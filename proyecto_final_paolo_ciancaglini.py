# -*- coding: utf-8 -*-
"""Proyecto final - Paolo Ciancaglini.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rt6v-cbD_nAZr_jjIFvP7AL2fwow3ngu
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV 
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, RandomizedSearchCV
from sklearn.feature_selection import RFE, RFECV  
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm

#como el objetivo es solamente la prediccion, nos quedamos con esas dos variables dicotomicas (race/ethnicity), que son utiles para predecir valores de la variable objetivo. 
mathfinalgrade=pd.read_csv('/content/drive/My Drive/Colab Notebooks/student-mat.csv')
mathfinalgrade.head()

vcat = list(mathfinalgrade[['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']])
vnum = set(list(mathfinalgrade)).difference(set(vcat)).difference({'G3'})

vcat

def modelos(X, Y, norm=False, cv=5):
  regs = cross_val_score(LinearRegression(normalize=norm), X, Y, scoring='neg_mean_squared_error', cv = cv)
  ar = RidgeCV(normalize = norm).fit(X, Y).alpha_
  ridges = cross_val_score(Ridge(alpha=ar, normalize=norm), X, Y, scoring='neg_mean_squared_error', cv = cv)
  al = LassoCV(normalize = norm).fit(X, Y).alpha_
  lasos = cross_val_score(Lasso(alpha=ar, normalize=norm), X, Y, scoring='neg_mean_squared_error', cv = cv)
  ae = ElasticNetCV(normalize=norm).fit(X, Y).alpha_
  ens = cross_val_score(ElasticNet(alpha=ae, normalize=norm), X, Y, scoring='neg_mean_squared_error', cv = cv)
  print('reg lineal: ', np.mean(regs))
  print('hiperparam ridge: ', ar)
  print('reg ridge: ', np.mean(ridges))
  print('hiperparam lasso: ', al)
  print('reg lasso: ', np.mean(lasos))
  print('hiperparam Elastic Net: ', ae)
  print('reg Elastic Net: ', np.mean(ens))



Dicos = pd.get_dummies(mathfinalgrade[vcat],drop_first=False) 
X = pd.concat([Dicos,mathfinalgrade[vnum]],axis=1)
Y = mathfinalgrade['G3']

modelos(X,Y)

"""# Selección de variables

## Selección por filtrado basado en correlaciones
"""

plt.figure(figsize=(35,30)) 
R = pd.concat([Y,X],1).corr() 
sns.heatmap(R, annot=True, cmap=plt.cm.Reds)
plt.show()

from google.colab import drive
drive.mount('/content/drive')

corrs = abs(R['G3']).sort_values(ascending=False)
Xr = X[corrs[1:21].index.tolist()] 
# se usan las variables con datos mayores a 0.10 en su correlacion con math score, que son 20. 
Xr

corrs

modelos(X,Y) 
#podemos ver que en nuestro modelo normal, el mejor modelo es de ElasticNet, con valor de 3.74.
#Nuestra regresion lineal esta muy desalineada

modelos(Xr,Y) #nuestra regresion lineal ahora nos muestra datos mas cercanos a sus pares. 
#En el modelo Selección por filtrado basado en correlaciones,  la mejor es la MSE de ElasticNet. Pero la ElasticNet del modelo normal es mejor que la del modelo Selección por filtrado basado en correlaciones.
#Tambien se puede observar que despues de seleccionar variables, la regresion lineal se asemeja mucho a los MSE de las otras regresiones.

"""## Selección stepwise backward"""

#metodo Stepside BACKWARD 
cols = list(X.columns)  
while (len(cols)>0):
  X_1 = sm.add_constant(X[cols]) 
  modelo = sm.OLS(Y, X_1).fit() 
  if(max(modelo.pvalues)>0.05): 
    cols.remove(modelo.pvalues[1:].idxmax()) 
  else:
    break
print(cols)

modelos(X[cols],Y) #Las MSE del metodo stepwise backward son un poco mas bajas que en los anteriores modelos. La regression Lasso se mantiene con un valor de 11.
#SE COMPARA el resultado con los resultados de los modelos anteriores. Hasta ahora, el ridge del metodo stepwise backward es el mejor (MSE=3.7314)

"""## Recursive Feature Selection"""

rfecv = RFECV(LinearRegression(), scoring='neg_mean_squared_error', cv=5).fit(X, Y)
rfecv.n_features_
rfecv

rfecv.grid_scores_ #aqui podemos observar todos los MSE Negativos.
#a medida que se utilizan mas variables, aumenta el MSE.

plt.figure()
plt.xlabel("Número de atributos seleccionados")
plt.ylabel("Error cuadrático medio negativo")
plt.plot(range(1, len(rfecv.grid_scores_) +1), rfecv.grid_scores_)
plt.show()
#aqui podemos observar que los incrementos son apenas perceptibles.
#Si, hay un salto significativo de la 6ta a la 8va variable. Si queremos reducir el numero de variables de atributos, podemos usar 8 variables.

rfe = RFE(LinearRegression(), 41) #con el numero de variables optimo (8), llamo a RFE. 
X_rfe = rfe.fit_transform(X, Y) #LUEGO me va a devolver la matriz X con las variables seleccionadas con este metodo. Nos da un resultado en una matriz.
print(rfe.support_) #me devuelve True si es la variable seleccionada, y False si no lo es. 
print(rfe.ranking_) #todos los True son 1. los False no son 1, dando asi un numero segun la importancia de las variables. La variable mas importante despues del 1 es 2, luego 3. 
#en este caso, vamos a utilizar solamente las variables True (1, las mas importantes)
X_rfe

modelos(X_rfe,Y)
# En el modelo Recursive Feature Selection, la mejor MSE es la de reg ridge (MSE=3.97)
#Sin embargo, el ridge del metodo stepwise backward es el mejor (MSE=3.7314) hasta el momento.

"""## Selección por método Lasso"""

regl = LassoCV().fit(X,Y)
print(regl.alpha_, regl.score(X,Y))
cf = pd.Series(regl.coef_, index = X.columns) 
varsec = cf[cf!=0].index.tolist()
varsec #tenemos 7 variables seleccionadas por lasso

regl.coef_



modelos(X[varsec],Y)  #usamos las 7 variables seleccionadas por varsec.
# En el modelo Lasso, la mejor MSE es la de reg ridge (MSE=3.6556)
#El ridge del metodo stepwise backward (MSE=3.7314), es mejorado por la reg Ridge del metodo Lasso.

"""## Lasso normalizado"""

regln = LassoCV(normalize=True).fit(X,Y)
print(regln.alpha_, regln.score(X,Y)) 
cfn = pd.Series(regln.coef_, index = X.columns)
varsecn = cfn[cfn!=0].index.tolist() 
varsecn #podemos ver que aqui el lasso normalizado ha seleccionado 8 variables. Usando esta normalizacion, tenemos otro conjunto de variables.

modelos(X[varsecn],Y,norm=True)
# En el modelo Lasso Normalizado, la mejor MSE es la de reg lineal (MSE=3.69)
#Sin embargo, el ridge del metodo Lasso es el mejor (MSE=3.6556) hasta el momento.

"""## Elastic Net"""

elast = ElasticNetCV().fit(X,Y)
print(elast.alpha_, elast.score(X,Y))
cfe = pd.Series(elast.coef_, index = X.columns)
varsee = cfe[cfe!=0].index.tolist()
varsee #en este caso, tenemos 8 variables seleccionadas por el metodo ElasticNet.

modelos(X[varsee],Y)
# En el modelo ElasticNet, la mejor MSE es la de reg ElasticNet (MSE=3.6873)
#Sin embargo, el ridge del metodo Lasso es el mejor (MSE=3.6556) hasta el momento.

"""CONCLUSION: Se construyó un algoritmo para estimar 
varios modelos de regresión, con el objetivo de predecir la nota final (G3). Segun la comparación de los MSE de los diferentes modelos, la regresión ridge del metodo Lasso es el mejor (MSE=3.6556), siendo el MSE mas bajo de todos los modelos.

# Evaluación del modelo (parte 2)

## Variables del modelo
"""

vars_cat_mod = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']
vars_cuant_mod = ['Dalc', 'Fedu', 'G1', 'G2', 'Medu', 'Walc', 'absences', 'age', 'failures', 'famrel', 'freetime', 'goout', 'health', 'studytime','traveltime']
X = mathfinalgrade[vars_cuant_mod]
Y = mathfinalgrade.G3

"""## Conjuntos de entrenamiento y de prueba"""

Xen, Xpr, Yen, Ypr = train_test_split(X, Y)
np.shape(Xen)   
np.shape(Xpr)
Xen, Xpr, Yen, Ypr = train_test_split(X, Y, test_size=0.2)

"""## Evaluación dentro y fuera de la muestra de entrenamiento"""

reg = LinearRegression()
reg.fit(Xen, Yen)
Yen_p = reg.predict(Xen)
Ypr_p = reg.predict(Xpr)

print('MSE entrenamiento:', mean_squared_error(Yen_p, Yen), '        MSE prueba', mean_squared_error(Ypr_p, Ypr))

"""# Validación cruzada"""

reg = LinearRegression()
mses = cross_val_score(reg, X, Y, scoring='neg_mean_squared_error', cv=5)
mses = -mses
print(mses)
print(np.mean(mses))

"""# Métodos de contracción

## Regresión Ridge
"""

lambdas = {'alpha': np.arange(1, 301)}
ridges = GridSearchCV(Ridge(), lambdas, scoring='neg_mean_squared_error', cv=5)
ridges.fit(X, Y)
print(ridges.best_params_)
print(ridges.best_score_)

#el mejor parametro de lambda es el 99. 
#Comparamos el dato resultado 3.7721 (mejor MSE) con el dato de validacion cruzada 3.8430 (MSE mean), y vemos que nuestro nuevo dato de estimacion es un poco mas pequenho. Tiene un error cuadratico medio menor.

"""## Regresión Lasso"""

alfas = {'alpha': np.arange(1, 501)}
lasos = GridSearchCV(Lasso(), alfas, scoring='neg_mean_squared_error', cv=5)
lasos.fit(X, Y) 
print(lasos.best_params_)
print(lasos.best_score_)
#en el resultado podemos observar que mientras mas subimos el rango, nuestro valor mejor MSE baja cada vez sube mas.
#el mejor parametro es el 1, con MSE de 3.89

"""# Comparando varios modelos

## Función comparativa
"""

def modelo2(X,Y):
  reg = LinearRegression()
  mses = cross_val_score(reg, X, Y, scoring='neg_mean_squared_error', cv=5)
  lambdas = {'alpha': np.arange(1, 101)}
  rid = Ridge(normalize = True) 
  ridges = GridSearchCV(rid, lambdas, scoring='neg_mean_squared_error', cv=5)
  ridges.fit(X, Y)
  las = Lasso(normalize = True) 
  lasos = GridSearchCV(las, lambdas, scoring='neg_mean_squared_error', cv=5,)
  lasos.fit(X, Y)
  print('reg lineal: ', np.mean(mses))
  print('hiperparam ridge: ', ridges.best_params_)
  print('reg ridge: ',ridges.best_score_)
  print('hiperparam lasso: ', lasos.best_params_)
  print('reg lasso: ',lasos.best_score_)

modelo2(X,Y)

"""## Añadiendo variables Dicotómicas"""

Dicos = pd.get_dummies(mathfinalgrade[vars_cat_mod], drop_first = False) #Transformamos las variables categoricas en dicotomicas. 
X = pd.concat([Dicos, mathfinalgrade[vars_cuant_mod]], axis=1) 
Y = mathfinalgrade.G3
modelo2(X, Y)

"""## Añadiendo variables polinomiales"""

Y = mathfinalgrade.G3
for i in range(1,4):
  Xi = PolynomialFeatures(i).fit_transform(mathfinalgrade[vars_cuant_mod]) 
  print('Modelo polinomial para grado', i) 
  modelo2(Xi, Y)

  #Podemos observar que el mejor MSE es del modelo regresion lineal, del modelo polinomial de grado 1 (MSE=3.8430). En los modelos de grado 2 y 3, la regresion ridge es la mejor de las 3 regresiones.

"""## Combinando dicotómicas y términos polinomiales"""

Dicos = pd.get_dummies(mathfinalgrade[vars_cat_mod], drop_first = True) 
Y = mathfinalgrade.G3
for i in range(1,4):
  Xi = PolynomialFeatures(i).fit_transform(mathfinalgrade[vars_cuant_mod])
  Xi = pd.DataFrame(Xi, index = mathfinalgrade.index.values) 
  Xi = pd.concat([Dicos, Xi], axis=1) 
  print('Modelo con dicotómicas y polinomial de grado', i)
  modelo2(Xi, Y)
#Comparamos los modelos con dicotómicas y polinomial con diferentes grados.
#Podemos observar que el mejor MSE es del modelo regresion ridge, de grado 3 (MSE= 4.1397)







